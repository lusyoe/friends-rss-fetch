import logging
import os
from dotenv import load_dotenv
from pyxxl import ExecutorConfig, PyxxlRunner
from pyxxl.ctx import g
import pymysql
import feedparser
from datetime import datetime

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®pyxxlæ¡†æ¶çš„æ—¥å¿—
pyxxl_logger = logging.getLogger('pyxxl')
pyxxl_logger.setLevel(logging.INFO)

# åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# è®¾ç½®æ—¥å¿—æ ¼å¼
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)

# æ·»åŠ å¤„ç†å™¨åˆ°pyxxl logger
pyxxl_logger.addHandler(console_handler)

# å¦‚æœxxl-adminå¯ä»¥ç›´è¿executorçš„ipï¼Œå¯ä»¥ä¸å¡«å†™executor_listen_host
config = ExecutorConfig(
    xxl_admin_baseurl=os.getenv("XXL_ADMIN_BASEURL", "http://xxljob.luhome.com/xxl-job-admin/api/"),
    executor_app_name=os.getenv("EXECUTOR_APP_NAME", "python-rss-fetch-executor"),
    executor_url=os.getenv("EXECUTOR_URL", "http://192.168.10.1:9999"),
    executor_listen_host=os.getenv("EXECUTOR_LISTEN_HOST", "0.0.0.0"),
    executor_listen_port=int(os.getenv("EXECUTOR_LISTEN_PORT", "9999")),
    access_token=os.getenv("ACCESS_TOKEN", "default_token"),
)

# æ•°æ®åº“è¿æ¥é…ç½®
DB_CONFIG = {
    "host": os.getenv("DB_HOST", "127.0.0.1"),
    "user": os.getenv("DB_USER", "root"),
    "password": os.getenv("DB_PASSWORD", ""),
    "database": os.getenv("DB_DATABASE", "blog"),
    "charset": os.getenv("DB_CHARSET", "utf8mb4")
}

app = PyxxlRunner(config)

def get_friend_links(conn):
    with conn.cursor() as cursor:
        cursor.execute("""
            SELECT id, rss_url 
            FROM friend_links 
            WHERE rss_url IS NOT NULL 
            AND rss_url != '' 
            AND is_active = 1
        """)
        return cursor.fetchall()

def fetch_rss_articles(rss_url):
    feed = feedparser.parse(rss_url)
    articles = []
    
    # æ£€æŸ¥feedç±»å‹å¹¶è®°å½•
    feed_type = "æœªçŸ¥"
    if hasattr(feed, 'version'):
        if 'rss' in feed.version.lower():
            feed_type = "RSS"
        elif 'atom' in feed.version.lower():
            feed_type = "Atom"
    
    g.logger.info(f"    æ£€æµ‹åˆ°Feedç±»å‹: {feed_type}")
    
    for entry in feed.entries:
        title = entry.get('title', '')
        link = entry.get('link', '')
        
        # å‘å¸ƒæ—¶é—´å…¼å®¹RSSå’ŒAtomæ ¼å¼
        published_time = None
        
        # æ ¹æ®feedç±»å‹é€‰æ‹©åˆé€‚çš„æ—¶é—´å­—æ®µ
        if feed_type == "RSS":
            # RSSæ ¼å¼ä¼˜å…ˆä½¿ç”¨pubDateå’Œpublishedå­—æ®µ
            time_fields = [
                ('pubDate_parsed', 'pubDate'),
                ('published_parsed', 'published')
            ]
        elif feed_type == "Atom":
            # Atomæ ¼å¼ä¼˜å…ˆä½¿ç”¨updatedå’Œpublishedå­—æ®µ
            time_fields = [
                ('updated_parsed', 'updated'),
                ('published_parsed', 'published'),
                ('created_parsed', 'created'),
                ('modified_parsed', 'modified')
            ]
        else:
            # æœªçŸ¥æ ¼å¼ï¼Œå°è¯•æ‰€æœ‰å­—æ®µ
            time_fields = [
                ('published_parsed', 'published'),
                ('updated_parsed', 'updated'),
                ('pubDate_parsed', 'pubDate'),
                ('created_parsed', 'created'),
                ('modified_parsed', 'modified')
            ]
        
        for parsed_field, raw_field in time_fields:
            if hasattr(entry, parsed_field) and getattr(entry, parsed_field):
                try:
                    parsed_time = getattr(entry, parsed_field)
                    published_time = datetime(*parsed_time[:6])
                    break
                except Exception:
                    continue
        
        # å¦‚æœæ²¡æœ‰è§£æçš„æ—¶é—´ï¼Œå°è¯•è§£æåŸå§‹æ—¶é—´å­—ç¬¦ä¸²
        if published_time is None:
            if feed_type == "RSS":
                raw_time_fields = ['pubDate', 'published']
            elif feed_type == "Atom":
                raw_time_fields = ['updated', 'published', 'created', 'modified']
            else:
                raw_time_fields = ['published', 'updated', 'pubDate', 'created', 'modified']
            
            for field in raw_time_fields:
                raw_time = entry.get(field, '')
                if raw_time:
                    try:
                        # feedparserä¼šè‡ªåŠ¨å°è¯•è§£ææ—¶é—´
                        if hasattr(entry, f'{field}_parsed') and getattr(entry, f'{field}_parsed'):
                            parsed_time = getattr(entry, f'{field}_parsed')
                            published_time = datetime(*parsed_time[:6])
                            break
                    except Exception:
                        continue
        
        articles.append({
            "title": title,
            "link": link,
            "created_at": published_time
        })
    
    return articles

def article_exists(conn, friend_id, link):
    with conn.cursor() as cursor:
        sql = "SELECT 1 FROM friend_rss_articles WHERE friend_id=%s AND link=%s LIMIT 1"
        cursor.execute(sql, (friend_id, link))
        return cursor.fetchone() is not None

def save_articles(conn, friend_id, articles):
    to_insert = []
    for article in articles:
        if not article['title'] or not article['link']:
            continue
        if article_exists(conn, friend_id, article['link']):
            continue
        to_insert.append((
            friend_id,
            article['title'],
            article['link'],
            article['created_at'] if article['created_at'] else None
        ))
        if len(to_insert) == 100:
            with conn.cursor() as cursor:
                sql = """
                    INSERT INTO friend_rss_articles (friend_id, title, link, created_at)
                    VALUES (%s, %s, %s, %s)
                """
                cursor.executemany(sql, to_insert)
            conn.commit()
            to_insert = []
    # æ’å…¥å‰©ä½™ä¸è¶³100æ¡çš„éƒ¨åˆ†
    if to_insert:
        with conn.cursor() as cursor:
            sql = """
                INSERT INTO friend_rss_articles (friend_id, title, link, created_at)
                VALUES (%s, %s, %s, %s)
            """
            cursor.executemany(sql, to_insert)
        conn.commit()

def update_fetch_failed_count(conn, friend_id, failed_count):
    """æ›´æ–°fetch_failed_countå­—æ®µï¼Œå½“è¾¾åˆ°3æ¬¡æ—¶åœç”¨å‹é“¾"""
    with conn.cursor() as cursor:
        if failed_count >= 3:
            # å½“å¤±è´¥æ¬¡æ•°è¾¾åˆ°3æ¬¡æ—¶ï¼Œåœç”¨å‹é“¾
            sql = "UPDATE friend_links SET fetch_failed_count = %s, is_active = 0 WHERE id = %s"
            cursor.execute(sql, (failed_count, friend_id))
            g.logger.info(f"  ğŸš« friend_id={friend_id} å¤±è´¥æ¬¡æ•°è¾¾åˆ°3æ¬¡ï¼Œå·²åœç”¨")
        else:
            # æ­£å¸¸æ›´æ–°å¤±è´¥è®¡æ•°
            sql = "UPDATE friend_links SET fetch_failed_count = %s WHERE id = %s"
            cursor.execute(sql, (failed_count, friend_id))
    conn.commit()

def reset_fetch_failed_count(conn, friend_id):
    """æŠ“å–æˆåŠŸåé‡ç½®å¤±è´¥è®¡æ•°"""
    with conn.cursor() as cursor:
        sql = "UPDATE friend_links SET fetch_failed_count = 0 WHERE id = %s"
        cursor.execute(sql, (friend_id,))
    conn.commit()

def insert_fetch_log(conn, friend_id, rss_url, status, http_status=None, message=None):
    """å†™å…¥æŠ“å–æ—¥å¿—åˆ°friend_rss_fetch_logsè¡¨"""
    with conn.cursor() as cursor:
        sql = """
            INSERT INTO friend_rss_fetch_logs (friend_id, rss_url, status, http_status, message, fetched_at)
            VALUES (%s, %s, %s, %s, %s, %s)
        """
        cursor.execute(sql, (
            friend_id,
            rss_url,
            status,
            http_status,
            message,
            datetime.now()
        ))
    conn.commit()

def insert_fetch_logs_batch(conn, logs):
    """æ‰¹é‡å†™å…¥æŠ“å–æ—¥å¿—åˆ°friend_rss_fetch_logsè¡¨"""
    if not logs:
        return
    with conn.cursor() as cursor:
        sql = """
            INSERT INTO friend_rss_fetch_logs (friend_id, rss_url, status, http_status, message, fetched_at)
            VALUES (%s, %s, %s, %s, %s, %s)
        """
        data = [
            (
                log['friend_id'],
                log['rss_url'],
                log['status'],
                log.get('http_status'),
                log.get('message'),
                log['fetched_at']
            ) for log in logs
        ]
        cursor.executemany(sql, data)
    conn.commit()


@app.register(name="rss_fetch")
async def rss_fetch():
    """
    å®šæ—¶è·å–rssè®¢é˜…ä¿¡æ¯
    """
    conn = pymysql.connect(**DB_CONFIG)
    try:
        links = get_friend_links(conn)
        g.logger.info(f"å…±æ‰¾åˆ° {len(links)} ä¸ªæ¿€æ´»çš„å‹é“¾æœ‰ RSS")
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        success_count = 0
        failed_count = 0
        zero_articles_count = 0
        zero_articles_links = []
        failed_links = []
        deactivated_links = []
        fetch_logs = []  # æ–°å¢æ—¥å¿—æ”¶é›†åˆ—è¡¨
        
        for friend_id, rss_url in links:
            g.logger.info(f"\næŠ“å– friend_id={friend_id} çš„ RSS: {rss_url}")
            try:
                articles = fetch_rss_articles(rss_url)
                g.logger.info(f"  å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
                
                if len(articles) == 0:
                    # è®°å½•å‘ç°0ç¯‡æ–‡ç« çš„é“¾æ¥
                    zero_articles_count += 1
                    zero_articles_links.append({
                        'friend_id': friend_id,
                        'rss_url': rss_url
                    })
                    g.logger.info("  âš ï¸  å‘ç°0ç¯‡æ–‡ç« ")
                    # å¢åŠ å¤±è´¥è®¡æ•°
                    failed_links.append(friend_id)
                    # æ”¶é›†æ—¥å¿—
                    fetch_logs.append({
                        'friend_id': friend_id,
                        'rss_url': rss_url,
                        'status': 'zero_articles',
                        'http_status': None,
                        'message': 'å‘ç°0ç¯‡æ–‡ç« ',
                        'fetched_at': datetime.now()
                    })
                else:
                    g.logger.info("  æ­£åœ¨å…¥åº“...")
                    save_articles(conn, friend_id, articles)
                    g.logger.info("  å…¥åº“å®Œæˆ")
                    reset_fetch_failed_count(conn, friend_id)  # æŠ“å–æˆåŠŸåæ¸…ç©ºå¤±è´¥æ¬¡æ•°
                    success_count += 1
                    # æ”¶é›†æ—¥å¿—
                    fetch_logs.append({
                        'friend_id': friend_id,
                        'rss_url': rss_url,
                        'status': 'success',
                        'http_status': None,
                        'message': f'æˆåŠŸæŠ“å–{len(articles)}ç¯‡æ–‡ç« ',
                        'fetched_at': datetime.now()
                    })
                    
            except Exception as e:
                error_msg = str(e)
                g.logger.error(f"  âŒ æŠ“å–æˆ–å…¥åº“å¤±è´¥: {error_msg}")
                failed_count += 1
                # å¢åŠ å¤±è´¥è®¡æ•°
                failed_links.append(friend_id)
                # æ”¶é›†æ—¥å¿—
                fetch_logs.append({
                    'friend_id': friend_id,
                    'rss_url': rss_url,
                    'status': 'fail',
                    'http_status': None,
                    'message': error_msg,
                    'fetched_at': datetime.now()
                })
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„fetch_failed_countå­—æ®µ
        g.logger.info(f"\næ­£åœ¨æ›´æ–°æ•°æ®åº“ä¸­çš„fetch_failed_countå­—æ®µ...")
        for friend_id in failed_links:
            # è·å–å½“å‰å¤±è´¥è®¡æ•°å¹¶åŠ 1
            with conn.cursor() as cursor:
                cursor.execute("SELECT COALESCE(fetch_failed_count, 0) FROM friend_links WHERE id = %s", (friend_id,))
                current_count = cursor.fetchone()[0]
                new_count = current_count + 1
                update_fetch_failed_count(conn, friend_id, new_count)
                
                if new_count >= 3:
                    deactivated_links.append(friend_id)
                else:
                    g.logger.info(f"  æ›´æ–° friend_id={friend_id}: fetch_failed_count = {new_count}")
        
        # å¾ªç¯ç»“æŸåç»Ÿä¸€æ‰¹é‡æ’å…¥æ—¥å¿—
        insert_fetch_logs_batch(conn, fetch_logs)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        g.logger.info(f"\n{'='*50}")
        g.logger.info("æŠ“å–å®Œæˆç»Ÿè®¡:")
        g.logger.info(f"æ¿€æ´»çš„å‹é“¾æ•°: {len(links)}")
        g.logger.info(f"æˆåŠŸæŠ“å–: {success_count}")
        g.logger.info(f"æŠ“å–å¤±è´¥: {failed_count}")
        g.logger.info(f"å‘ç°0ç¯‡æ–‡ç« : {zero_articles_count}")
        g.logger.info(f"éœ€è¦æ›´æ–°fetch_failed_countçš„é“¾æ¥æ•°: {len(failed_links)}")
        g.logger.info(f"å› å¤±è´¥æ¬¡æ•°è¿‡å¤šè€Œåœç”¨çš„é“¾æ¥æ•°: {len(deactivated_links)}")
        
        if zero_articles_links:
            g.logger.info(f"\nå‘ç°0ç¯‡æ–‡ç« çš„é“¾æ¥:")
            for link in zero_articles_links:
                g.logger.info(f"  - friend_id={link['friend_id']}: {link['rss_url']}")
            g.logger.info(f"å‘ç°0ç¯‡æ–‡ç« çš„é“¾æ¥æ•°: {len(zero_articles_links)}")
        if deactivated_links:
            g.logger.info(f"\nå› å¤±è´¥æ¬¡æ•°è¿‡å¤šè€Œåœç”¨çš„é“¾æ¥:")
            for friend_id in deactivated_links:
                g.logger.info(f"  - friend_id={friend_id}")
        
        g.logger.info(f"{'='*50}")
        
    finally:
        conn.close()


if __name__ == "__main__":
    app.run_executor()
